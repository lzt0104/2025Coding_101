{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyRjTreuVbGb"
   },
   "source": [
    "*數據準備和預處理*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15166,
     "status": "ok",
     "timestamp": 1717078470616,
     "user": {
      "displayName": "R-J HONG",
      "userId": "02087882872863851149"
     },
     "user_tz": -480
    },
    "id": "YPvqkQ4M3V87",
    "outputId": "1dfdf6fe-2299-4f57-966b-b1de6ee93efd",
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch==2.5.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (2.5.1)\n",
      "Requirement already satisfied: opencv-python==4.10.0.84 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.10.0.84)\n",
      "Requirement already satisfied: numpy==1.26.2 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.26.2)\n",
      "Requirement already satisfied: torchvision==0.20.1 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (0.20.1)\n",
      "Requirement already satisfied: pyttsx3==2.98 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (2.98)\n",
      "Requirement already satisfied: Pillow==10.1.0 in /opt/conda/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (10.1.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (4.9.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch==2.5.1->-r requirements.txt (line 1)) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch==2.5.1->-r requirements.txt (line 1)) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch==2.5.1->-r requirements.txt (line 1)) (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/2025Coding_101\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XHpxhiNaUdmP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# 圖像預處理\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_file, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.labels = pd.read_csv(labels_file)['risk'].values\n",
    "        #讀tarin內的資料夾\n",
    "        self.image_paths = sorted(\n",
    "                  os.path.join(image_dir, i)\n",
    "                  for i in os.listdir(image_dir)\n",
    "                  )\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_paths = self.image_paths[idx]\n",
    "        img_list = []\n",
    "        for image_path in os.listdir(image_paths):\n",
    "          image = cv2.imread(os.path.join(image_paths, image_path))\n",
    "          image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "          if self.transform:\n",
    "              image = self.transform(image)\n",
    "          img_list.append(image)\n",
    "        while len(img_list)<169:\n",
    "          img_list.append(image)\n",
    "        image_tensor = torch.stack(img_list)  # Convert list of images to tensor\n",
    "        label = torch.stack([torch.tensor(self.labels[idx])])\n",
    "        return image_tensor, label\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "dataset = AccidentDataset(\n",
    "    image_dir='freeway/train',\n",
    "    labels_file='freeway/freeway_train.csv',\n",
    "    transform=transform)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USqNuqxLVeUi"
   },
   "source": [
    "分割訓練集和測試集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uFAERaiaVf4S",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "train_indices, test_indices = train_test_split(list(range(len(dataset))), test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = Subset(dataset, train_indices)\n",
    "test_dataset = Subset(dataset, test_indices)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SqtDh31_kIhw"
   },
   "source": [
    "#分隔線"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0mfe2NG6VhYx"
   },
   "source": [
    "構建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#0302 New\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedCNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=(224, 224)):\n",
    "        super(EnhancedCNN_LSTM, self).__init__()\n",
    "        # Assuming input size is 224x224 (common for CNNs), adjust if different\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # Depthwise Separable Conv to reduce parameters\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.conv2 = self._make_dw_conv(64, 128)\n",
    "        self.conv3 = self._make_dw_conv(128, 128)\n",
    "        self.conv4 = self._make_dw_conv(128, 256)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=1, stride=1, bias=False)  # Pointwise conv for efficiency\n",
    "        self.bn5 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Compute CNN output size dynamically\n",
    "        self._cnn_output_size = self._get_cnn_output_size()\n",
    "        self.fc1 = nn.Linear(self._cnn_output_size, 512)\n",
    "        self.dropout = nn.Dropout(0.3)  # Add dropout for regularization\n",
    "\n",
    "        # LSTM with bidirectional for better sequence modeling\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, \n",
    "                           batch_first=True, bidirectional=True)\n",
    "        self.fc2 = nn.Linear(256 * 2, num_classes)  # *2 due to bidirectional\n",
    "\n",
    "    def _make_dw_conv(self, in_channels, out_channels):\n",
    "        \"\"\"Depthwise separable convolution block\"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, kernel_size=3, stride=1, padding=1, \n",
    "                     groups=in_channels, bias=False),  # Depthwise\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, bias=False),  # Pointwise\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def _get_cnn_output_size(self):\n",
    "        \"\"\"Calculate the flattened size after CNN layers\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, 3, *self.input_size)  # Dummy input\n",
    "            x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "            x = self.pool(self.conv2(x))\n",
    "            x = self.pool(self.conv3(x))\n",
    "            x = self.pool(self.conv4(x))\n",
    "            x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "            return x.view(1, -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "\n",
    "        # CNN with residual-like behavior via careful design\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(c_in))))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.pool(self.conv3(x))\n",
    "        x = self.pool(self.conv4(x))\n",
    "        x = self.pool(F.relu(self.bn5(self.conv5(x))))\n",
    "\n",
    "        # Flatten and fully connected\n",
    "        x = x.view(batch_size * timesteps, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # LSTM\n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc2(x[:, -1, :])  # Take the last timestep\n",
    "        return torch.sigmoid(x)  # Assuming binary/multilabel classification\n",
    "\n",
    "# Instantiate the model\n",
    "model = EnhancedCNN_LSTM(num_classes=1, input_size=(224, 224))\n",
    "\n",
    "# Optional: Enable mixed precision training\n",
    "model = model.half()  # Use FP16 for lower memory usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "t_UQpFSwVjAJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN_LSTM, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        self.fc1 = nn.Linear(256 * 7 * 7, 512)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, batch_first=True)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        x = self.pool(F.relu(self.conv1(c_in)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = self.pool(F.relu(self.conv3(x)))\n",
    "        x = self.pool(F.relu(self.conv4(x)))\n",
    "        x = self.pool(F.relu(self.conv5(x)))\n",
    "        x = F.relu(self.fc1(x.view(batch_size * timesteps,-1)))\n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = torch.sigmoid(self.fc2(x[:, -1, :]))\n",
    "        return x\n",
    "model = CNN_LSTM(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "RKUNW1pNyE2u",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: torch in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: torchvision in /opt/conda/lib/python3.11/site-packages (0.20.1)\n",
      "Requirement already satisfied: torchaudio in /opt/conda/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.11/site-packages (from torch) (4.9.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.11/site-packages (from torch) (2023.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /opt/conda/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /opt/conda/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /opt/conda/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /opt/conda/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /opt/conda/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /opt/conda/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /opt/conda/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /opt/conda/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/conda/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.11/site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.11/site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch) (2.1.4)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "cQSTLkuoy9yT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Debb6TeSVk6Z"
   },
   "source": [
    "訓練模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-t1NCMhEVkAK",
    "outputId": "9b69602f-8ae1-461a-fe1d-dfdb6238b514",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/144 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m#images, labels = images.to(device), labels.to(device).float().view(-1, 1)  # 確保 labels 轉為 float\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#images, labels = model(images.to(device)), labels(images.to(device)).view(-1, 1)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#images, labels = images.to(device), labels.view(-1,1)\u001b[39;00m\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# 直接輸出 logits，讓 BCEWithLogitsLoss 處理 Sigmoid\u001b[39;00m\n\u001b[1;32m     19\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[3], line 60\u001b[0m, in \u001b[0;36mEnhancedCNN_LSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     57\u001b[0m c_in \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(batch_size \u001b[38;5;241m*\u001b[39m timesteps, C, H, W)\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# CNN with residual-like behavior via careful design\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m)\u001b[49m)))\n\u001b[1;32m     61\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[1;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(x))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/nn/modules/conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[1;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    548\u001b[0m     )\n\u001b[0;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in tqdm(train_loader):\n",
    "      #outputs = model(images.to(device))\n",
    "      images, labels = images.to(device), labels.to('cuda').float().view(-1, 1)\n",
    "      #images, labels = images.to(device), labels.to(device).float().view(-1, 1)  # 確保 labels 轉為 float\n",
    "      #images, labels = model(images.to(device)), labels(images.to(device)).view(-1, 1)\n",
    "      #images, labels = images.to(device), labels.view(-1,1)\n",
    "      optimizer.zero_grad()\n",
    "      outputs = model(images)  # 直接輸出 logits，讓 BCEWithLogitsLoss 處理 Sigmoid\n",
    "      loss = criterion(outputs, labels)\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      running_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:09<00:00, 31.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 1/150 個Epoch\n",
      "訓練損失: 0.7016, 驗證損失: 0.7035\n",
      "驗證準確率: 46.88%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 2/150 個Epoch\n",
      "訓練損失: 0.7002, 驗證損失: 0.6958\n",
      "驗證準確率: 50.35%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 3/150 個Epoch\n",
      "訓練損失: 0.6959, 驗證損失: 0.6967\n",
      "驗證準確率: 49.65%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 4/150 個Epoch\n",
      "訓練損失: 0.6959, 驗證損失: 0.6952\n",
      "驗證準確率: 48.26%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 5/150 個Epoch\n",
      "訓練損失: 0.6945, 驗證損失: 0.6948\n",
      "驗證準確率: 50.00%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 6/150 個Epoch\n",
      "訓練損失: 0.6947, 驗證損失: 0.7029\n",
      "驗證準確率: 46.53%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 7/150 個Epoch\n",
      "訓練損失: 0.6952, 驗證損失: 0.6941\n",
      "驗證準確率: 43.40%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 8/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6914\n",
      "驗證準確率: 52.78%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 33.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 9/150 個Epoch\n",
      "訓練損失: 0.6937, 驗證損失: 0.6961\n",
      "驗證準確率: 47.57%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 33.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 10/150 個Epoch\n",
      "訓練損失: 0.6947, 驗證損失: 0.6946\n",
      "驗證準確率: 47.22%\n",
      "學習率: 0.000500\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 11/150 個Epoch\n",
      "訓練損失: 0.6948, 驗證損失: 0.6932\n",
      "驗證準確率: 47.92%\n",
      "學習率: 0.000250\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 12/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6931\n",
      "驗證準確率: 53.12%\n",
      "學習率: 0.000250\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 13/150 個Epoch\n",
      "訓練損失: 0.6945, 驗證損失: 0.6949\n",
      "驗證準確率: 46.88%\n",
      "學習率: 0.000250\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 14/150 個Epoch\n",
      "訓練損失: 0.6935, 驗證損失: 0.6937\n",
      "驗證準確率: 45.49%\n",
      "學習率: 0.000125\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 15/150 個Epoch\n",
      "訓練損失: 0.6932, 驗證損失: 0.6930\n",
      "驗證準確率: 50.69%\n",
      "學習率: 0.000125\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 16/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6935\n",
      "驗證準確率: 47.92%\n",
      "學習率: 0.000125\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 17/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6931\n",
      "驗證準確率: 50.00%\n",
      "學習率: 0.000063\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 18/150 個Epoch\n",
      "訓練損失: 0.6928, 驗證損失: 0.6927\n",
      "驗證準確率: 55.21%\n",
      "學習率: 0.000063\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 19/150 個Epoch\n",
      "訓練損失: 0.6929, 驗證損失: 0.6929\n",
      "驗證準確率: 52.08%\n",
      "學習率: 0.000063\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 36.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 20/150 個Epoch\n",
      "訓練損失: 0.6938, 驗證損失: 0.6932\n",
      "驗證準確率: 50.35%\n",
      "學習率: 0.000031\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 21/150 個Epoch\n",
      "訓練損失: 0.6927, 驗證損失: 0.6928\n",
      "驗證準確率: 50.69%\n",
      "學習率: 0.000031\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 22/150 個Epoch\n",
      "訓練損失: 0.6932, 驗證損失: 0.6930\n",
      "驗證準確率: 52.08%\n",
      "學習率: 0.000031\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 23/150 個Epoch\n",
      "訓練損失: 0.6941, 驗證損失: 0.6933\n",
      "驗證準確率: 48.96%\n",
      "學習率: 0.000016\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 33.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 24/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6925\n",
      "驗證準確率: 53.12%\n",
      "學習率: 0.000016\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 25/150 個Epoch\n",
      "訓練損失: 0.6923, 驗證損失: 0.6928\n",
      "驗證準確率: 51.74%\n",
      "學習率: 0.000016\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 26/150 個Epoch\n",
      "訓練損失: 0.6929, 驗證損失: 0.6944\n",
      "驗證準確率: 46.53%\n",
      "學習率: 0.000008\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 27/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6932\n",
      "驗證準確率: 49.65%\n",
      "學習率: 0.000008\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 28/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6925\n",
      "驗證準確率: 51.39%\n",
      "學習率: 0.000008\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 32.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 29/150 個Epoch\n",
      "訓練損失: 0.6935, 驗證損失: 0.6940\n",
      "驗證準確率: 46.53%\n",
      "學習率: 0.000004\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 30/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6947\n",
      "驗證準確率: 45.83%\n",
      "學習率: 0.000004\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 31/150 個Epoch\n",
      "訓練損失: 0.6926, 驗證損失: 0.6932\n",
      "驗證準確率: 50.69%\n",
      "學習率: 0.000004\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 32/150 個Epoch\n",
      "訓練損失: 0.6932, 驗證損失: 0.6934\n",
      "驗證準確率: 50.35%\n",
      "學習率: 0.000002\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 33/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6931\n",
      "驗證準確率: 49.65%\n",
      "學習率: 0.000002\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 34/150 個Epoch\n",
      "訓練損失: 0.6944, 驗證損失: 0.6923\n",
      "驗證準確率: 53.47%\n",
      "學習率: 0.000002\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 35/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6934\n",
      "驗證準確率: 48.61%\n",
      "學習率: 0.000001\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 36/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6931\n",
      "驗證準確率: 51.04%\n",
      "學習率: 0.000001\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 37/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6931\n",
      "驗證準確率: 51.04%\n",
      "學習率: 0.000001\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 38/150 個Epoch\n",
      "訓練損失: 0.6938, 驗證損失: 0.6932\n",
      "驗證準確率: 52.08%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 33.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 39/150 個Epoch\n",
      "訓練損失: 0.6929, 驗證損失: 0.6942\n",
      "驗證準確率: 47.22%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 40/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6943\n",
      "驗證準確率: 47.57%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 41/150 個Epoch\n",
      "訓練損失: 0.6940, 驗證損失: 0.6925\n",
      "驗證準確率: 52.78%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 42/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6922\n",
      "驗證準確率: 54.51%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 43/150 個Epoch\n",
      "訓練損失: 0.6935, 驗證損失: 0.6926\n",
      "驗證準確率: 53.12%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 44/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6928\n",
      "驗證準確率: 51.74%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 45/150 個Epoch\n",
      "訓練損失: 0.6926, 驗證損失: 0.6924\n",
      "驗證準確率: 52.43%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:09<00:00, 31.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 46/150 個Epoch\n",
      "訓練損失: 0.6936, 驗證損失: 0.6941\n",
      "驗證準確率: 46.53%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 47/150 個Epoch\n",
      "訓練損失: 0.6932, 驗證損失: 0.6929\n",
      "驗證準確率: 52.08%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 48/150 個Epoch\n",
      "訓練損失: 0.6923, 驗證損失: 0.6932\n",
      "驗證準確率: 47.92%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 49/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6928\n",
      "驗證準確率: 51.74%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 50/150 個Epoch\n",
      "訓練損失: 0.6936, 驗證損失: 0.6931\n",
      "驗證準確率: 50.35%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 36.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 51/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6939\n",
      "驗證準確率: 47.22%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 52/150 個Epoch\n",
      "訓練損失: 0.6936, 驗證損失: 0.6938\n",
      "驗證準確率: 47.57%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 53/150 個Epoch\n",
      "訓練損失: 0.6936, 驗證損失: 0.6936\n",
      "驗證準確率: 48.26%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 34.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 54/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6935\n",
      "驗證準確率: 46.18%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 55/150 個Epoch\n",
      "訓練損失: 0.6934, 驗證損失: 0.6928\n",
      "驗證準確率: 51.74%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 33.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 56/150 個Epoch\n",
      "訓練損失: 0.6933, 驗證損失: 0.6935\n",
      "驗證準確率: 47.57%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:08<00:00, 35.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 57/150 個Epoch\n",
      "訓練損失: 0.6931, 驗證損失: 0.6933\n",
      "驗證準確率: 48.96%\n",
      "學習率: 0.000000\n",
      "GPU記憶體使用量: 0.25 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 259/288 [00:07<00:00, 34.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_3250/216947866.py\", line 129, in <module>\n",
      "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/tqdm/std.py\", line 1182, in __iter__\n",
      "    for obj in iterable:\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "  File \"/tmp/ipykernel_3250/216947866.py\", line 107, in __getitem__\n",
      "    img = self.transform(img)\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line 95, in __call__\n",
      "    img = t(img)\n",
      "          ^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/torchvision/transforms/transforms.py\", line None, in forward\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 2144, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1435, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1326, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1173, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 1088, in format_exception_as_a_whole\n",
      "    frames.append(self.format_record(record))\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 970, in format_record\n",
      "    frame_info.lines, Colors, self.has_colors, lvals\n",
      "    ^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/ultratb.py\", line 792, in lines\n",
      "    return self._sd.lines\n",
      "           ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/core.py\", line 734, in lines\n",
      "    pieces = self.included_pieces\n",
      "             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/core.py\", line 681, in included_pieces\n",
      "    pos = scope_pieces.index(self.executing_piece)\n",
      "                             ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/utils.py\", line 144, in cached_property_wrapper\n",
      "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
      "                                               ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/stack_data/core.py\", line 660, in executing_piece\n",
      "    return only(\n",
      "           ^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/executing/executing.py\", line 116, in only\n",
      "    raise NotOneValueFound('Expected one value, found 0')\n",
      "executing.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    }
   ],
   "source": [
    "#GPU 顯示版\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 設置記憶體優化環境變數，以減少記憶體碎片\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 輕量化的模型定義\n",
    "class EnhancedCNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=(128, 128)):  # 減少輸入尺寸\n",
    "        super(EnhancedCNN_LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # 輕量化的卷積神經網路 (CNN)\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)  # 減少濾波器數量\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.3)  # 添加Dropout以防止過擬合\n",
    "\n",
    "        # 計算扁平化後的大小，對於128x128輸入：128 / 8 = 16\n",
    "        self.fc1 = nn.Linear(128 * 16 * 16, 512)  # 減少全連接層大小\n",
    "        \n",
    "        # 輕量化的雙向LSTM\n",
    "        self.lstm = nn.LSTM(input_size=512, hidden_size=256, num_layers=2, \n",
    "                           batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc2 = nn.Linear(256 * 2, num_classes)  # 雙向LSTM輸出維度加倍\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        x = F.relu(self.bn1(self.conv1(c_in)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.pool(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(batch_size * timesteps, -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc2(x[:, -1, :])  # 輸出原始logits\n",
    "        return x\n",
    "\n",
    "# 實例化模型\n",
    "model = EnhancedCNN_LSTM(num_classes=1, input_size=(128, 128)).to(device)\n",
    "\n",
    "# 驗證模型參數為FP32\n",
    "for param in model.parameters():\n",
    "    assert param.dtype == torch.float32, \"模型參數必須為FP32\"\n",
    "\n",
    "# 損失函數與優化器\n",
    "criterion = nn.BCEWithLogitsLoss()  # 使用BCEWithLogitsLoss處理原始logits\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005, weight_decay=1e-4)  # 較低學習率\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)  # 動態調整學習率\n",
    "\n",
    "# 混合精度訓練的GradScaler\n",
    "scaler = GradScaler('cuda')\n",
    "\n",
    "# 資料增強\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # 調整圖像大小以匹配模型輸入\n",
    "    transforms.RandomHorizontalFlip(),  # 隨機水平翻轉\n",
    "    transforms.RandomRotation(10),  # 隨機旋轉10度\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),  # 隨機調整亮度與對比\n",
    "    transforms.ToTensor(),  # 轉為Tensor\n",
    "])\n",
    "\n",
    "# 示例資料集類（模擬PIL Image格式）\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self, num_samples, timesteps=10, transform=None):\n",
    "        self.num_samples = num_samples\n",
    "        self.timesteps = timesteps\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 模擬圖像數據：生成隨機NumPy陣列並轉為PIL Image\n",
    "        images = []\n",
    "        for t in range(self.timesteps):\n",
    "            # 生成隨機NumPy陣列，範圍0-255，模擬RGB圖像\n",
    "            img = np.random.randint(0, 256, (128, 128, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(img)  # 轉為PIL Image\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "        image = torch.stack(images)  # 堆疊為 [timesteps, 3, 128, 128]\n",
    "        label = torch.randint(0, 2, (1,)).float()  # 隨機生成0或1的標籤\n",
    "        return image, label\n",
    "\n",
    "# 創建訓練和驗證資料載入器\n",
    "train_dataset = DummyDataset(num_samples=1152, timesteps=10, transform=transform)  # 1152 / 4 = 288 batches\n",
    "val_dataset = DummyDataset(num_samples=288, timesteps=10, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# 訓練設置\n",
    "num_epochs = 150  # 訓練20個epoch\n",
    "accumulation_steps = 4  # 梯度累積步數\n",
    "batch_size = 1  # 減少批次大小以適應記憶體限制\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 設置模型為訓練模式\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device, dtype=torch.float32)  # 確保輸入為FP32\n",
    "        labels = labels.to(device, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "        with torch.amp.autocast(device_type='cuda'):  # 混合精度前向傳播\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / accumulation_steps  # 標準化損失以進行梯度累積\n",
    "\n",
    "        scaler.scale(loss).backward()  # 反向傳播\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            scaler.step(optimizer)  # 更新參數\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()  # 清空梯度\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps  # 累加損失\n",
    "\n",
    "    avg_train_loss = running_loss / len(train_loader)  # 計算平均訓練損失\n",
    "\n",
    "    # 驗證階段\n",
    "    model.eval()  # 設置模型為評估模式\n",
    "    val_preds, val_labels = [], []\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():  # 禁用梯度計算\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32).view(-1, 1)\n",
    "            with torch.amp.autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                val_loss += criterion(outputs, labels).item()\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5  # 預測二分類結果\n",
    "            val_preds.extend(preds.flatten())\n",
    "            val_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)  # 計算平均驗證損失\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds) * 100  # 修正為比較預測與標籤\n",
    "    scheduler.step(avg_val_loss)  # 根據驗證損失調整學習率\n",
    "\n",
    "    print(f'第 {epoch+1}/{num_epochs} 個Epoch')\n",
    "    print(f'訓練損失: {avg_train_loss:.4f}, 驗證損失: {avg_val_loss:.4f}')\n",
    "    print(f'驗證準確率: {val_accuracy:.2f}%')\n",
    "    print(f'學習率: {scheduler.optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print(f'GPU記憶體使用量: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model.state_dict(), 'enhanced_cnn_lstm.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存最佳模型，驗證準確率: 44.44%\n",
      "第 1/20 個 Epoch\n",
      "訓練損失: 0.0528, 訓練準確率: 51.22%\n",
      "驗證損失: 0.0459, 驗證準確率: 44.44%\n",
      "學習率: 0.000208\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存最佳模型，驗證準確率: 48.61%\n",
      "第 2/20 個 Epoch\n",
      "訓練損失: 0.0516, 訓練準確率: 49.91%\n",
      "驗證損失: 0.0466, 驗證準確率: 48.61%\n",
      "學習率: 0.000231\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 3/20 個 Epoch\n",
      "訓練損失: 0.0514, 訓練準確率: 49.13%\n",
      "驗證損失: 0.0438, 驗證準確率: 47.22%\n",
      "學習率: 0.000269\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存最佳模型，驗證準確率: 53.12%\n",
      "第 4/20 個 Epoch\n",
      "訓練損失: 0.0507, 訓練準確率: 45.83%\n",
      "驗證損失: 0.0445, 驗證準確率: 53.12%\n",
      "學習率: 0.000321\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 5/20 個 Epoch\n",
      "訓練損失: 0.0475, 訓練準確率: 50.78%\n",
      "驗證損失: 0.0442, 驗證準確率: 43.75%\n",
      "學習率: 0.000386\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 6/20 個 Epoch\n",
      "訓練損失: 0.0493, 訓練準確率: 46.70%\n",
      "驗證損失: 0.0433, 驗證準確率: 51.74%\n",
      "學習率: 0.000464\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "保存最佳模型，驗證準確率: 55.56%\n",
      "第 7/20 個 Epoch\n",
      "訓練損失: 0.0469, 訓練準確率: 50.61%\n",
      "驗證損失: 0.0431, 驗證準確率: 55.56%\n",
      "學習率: 0.000552\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 8/20 個 Epoch\n",
      "訓練損失: 0.0459, 訓練準確率: 51.65%\n",
      "驗證損失: 0.0468, 驗證準確率: 45.83%\n",
      "學習率: 0.000650\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 9/20 個 Epoch\n",
      "訓練損失: 0.0459, 訓練準確率: 48.87%\n",
      "驗證損失: 0.0440, 驗證準確率: 51.74%\n",
      "學習率: 0.000756\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 10/20 個 Epoch\n",
      "訓練損失: 0.0457, 訓練準確率: 49.05%\n",
      "驗證損失: 0.0494, 驗證準確率: 46.18%\n",
      "學習率: 0.000868\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 11/20 個 Epoch\n",
      "訓練損失: 0.0452, 訓練準確率: 49.65%\n",
      "驗證損失: 0.0437, 驗證準確率: 48.61%\n",
      "學習率: 0.000983\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 12/20 個 Epoch\n",
      "訓練損失: 0.0455, 訓練準確率: 50.09%\n",
      "驗證損失: 0.0433, 驗證準確率: 50.35%\n",
      "學習率: 0.001101\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 13/20 個 Epoch\n",
      "訓練損失: 0.0443, 訓練準確率: 52.52%\n",
      "驗證損失: 0.0447, 驗證準確率: 50.35%\n",
      "學習率: 0.001218\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 14/20 個 Epoch\n",
      "訓練損失: 0.0447, 訓練準確率: 50.87%\n",
      "驗證損失: 0.0437, 驗證準確率: 47.22%\n",
      "學習率: 0.001334\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 15/20 個 Epoch\n",
      "訓練損失: 0.0442, 訓練準確率: 51.22%\n",
      "驗證損失: 0.0445, 驗證準確率: 52.78%\n",
      "學習率: 0.001445\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 16/20 個 Epoch\n",
      "訓練損失: 0.0441, 訓練準確率: 50.26%\n",
      "驗證損失: 0.0434, 驗證準確率: 53.12%\n",
      "學習率: 0.001551\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 17/20 個 Epoch\n",
      "訓練損失: 0.0445, 訓練準確率: 47.31%\n",
      "驗證損失: 0.0433, 驗證準確率: 49.31%\n",
      "學習率: 0.001649\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 18/20 個 Epoch\n",
      "訓練損失: 0.0440, 訓練準確率: 50.00%\n",
      "驗證損失: 0.0437, 驗證準確率: 51.74%\n",
      "學習率: 0.001737\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 37.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 19/20 個 Epoch\n",
      "訓練損失: 0.0442, 訓練準確率: 49.31%\n",
      "驗證損失: 0.0444, 驗證準確率: 44.44%\n",
      "學習率: 0.001815\n",
      "GPU記憶體使用量: 0.93 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 288/288 [00:07<00:00, 38.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 20/20 個 Epoch\n",
      "訓練損失: 0.0445, 訓練準確率: 48.78%\n",
      "驗證損失: 0.0474, 驗證準確率: 52.43%\n",
      "學習率: 0.001880\n",
      "GPU記憶體使用量: 0.93 GB\n",
      "最終模型驗證準確率: 51.74%\n"
     ]
    }
   ],
   "source": [
    "#0302 Claude\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 設置記憶體優化環境變數，以減少記憶體碎片\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 強化版 CNN-LSTM 模型\n",
    "class EnhancedCNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size=(128, 128)):\n",
    "        super(EnhancedCNN_LSTM, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # 增強的卷積神經網路 (CNN) - 使用 ResNet 風格的殘差連接\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        \n",
    "        # 第一個殘差區塊\n",
    "        self.conv2a = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2a = nn.BatchNorm2d(64)\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2b = nn.BatchNorm2d(64)\n",
    "        self.downsample1 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 添加池化層使尺寸匹配\n",
    "        )\n",
    "        \n",
    "        # 第二個殘差區塊\n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3a = nn.BatchNorm2d(128)\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn3b = nn.BatchNorm2d(128)\n",
    "        self.downsample2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0)  # 添加池化層使尺寸匹配\n",
    "        )\n",
    "        \n",
    "        # 共享池化和 Dropout 層\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.dropout = nn.Dropout(0.4)  # 增加 Dropout 比例以減少過擬合\n",
    "        \n",
    "        # 空間注意力機制\n",
    "        self.attention_conv = nn.Conv2d(128, 1, kernel_size=1)\n",
    "        \n",
    "        # 計算特徵大小：128x128 經過 3 次 pool 後變為 16x16\n",
    "        # feature_size = input_size[0] // (2**3)\n",
    "        feature_size = 16\n",
    "        self.fc1 = nn.Linear(128 * feature_size * feature_size, 512)\n",
    "        \n",
    "        # 增強型雙向 LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=512, \n",
    "            hidden_size=256, \n",
    "            num_layers=2, \n",
    "            batch_first=True, \n",
    "            bidirectional=True, \n",
    "            dropout=0.4\n",
    "        )\n",
    "        \n",
    "        # 最終分類層\n",
    "        self.fc2 = nn.Linear(256 * 2, 128)  # 添加中間層\n",
    "        self.fc3 = nn.Linear(128, num_classes)\n",
    "        \n",
    "        # 批次正規化\n",
    "        self.bn_fc1 = nn.BatchNorm1d(512)\n",
    "        self.bn_fc2 = nn.BatchNorm1d(128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        # 初始卷積層\n",
    "        x = F.relu(self.bn1(self.conv1(c_in)))\n",
    "        \n",
    "        # 第一個殘差區塊\n",
    "        identity = self.downsample1(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = self.bn2b(self.conv2b(x))\n",
    "        x = F.relu(x + identity)\n",
    "        \n",
    "        # 第二個殘差區塊\n",
    "        identity = self.downsample2(x)\n",
    "        x = self.pool(x)\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = self.bn3b(self.conv3b(x))\n",
    "        x = F.relu(x + identity)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        # 空間注意力機制\n",
    "        attention = torch.sigmoid(self.attention_conv(x))\n",
    "        x = x * attention\n",
    "        \n",
    "        # Dropout 降低過擬合風險\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 扁平化\n",
    "        x = x.view(batch_size * timesteps, -1)\n",
    "        \n",
    "        # 全連接層\n",
    "        x = F.relu(self.bn_fc1(self.fc1(x)))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # 重塑為序列資料\n",
    "        x = x.view(batch_size, timesteps, -1)\n",
    "        \n",
    "        # 應用 LSTM\n",
    "        x, _ = self.lstm(x)\n",
    "        \n",
    "        # 使用最後一個時間步的輸出進行分類\n",
    "        x = F.relu(self.bn_fc2(self.fc2(x[:, -1, :])))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# 自定義損失函數\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, inputs, targets):\n",
    "        BCE_loss = self.bce(inputs, targets)\n",
    "        pt = torch.exp(-BCE_loss)\n",
    "        F_loss = self.alpha * (1-pt)**self.gamma * BCE_loss\n",
    "        return F_loss.mean()\n",
    "\n",
    "# 自定義資料集增強\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples, timesteps=10, transform=None, augment=False):\n",
    "        self.num_samples = num_samples\n",
    "        self.timesteps = timesteps\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 模擬時序圖像資料\n",
    "        images = []\n",
    "        for t in range(self.timesteps):\n",
    "            # 生成類模擬資料，實際應用中替換為真實圖像讀取\n",
    "            img = np.random.randint(0, 256, (128, 128, 3), dtype=np.uint8)\n",
    "            img = Image.fromarray(img)\n",
    "            \n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            images.append(img)\n",
    "            \n",
    "        image_seq = torch.stack(images)  # [timesteps, 3, 128, 128]\n",
    "        \n",
    "        # 模擬標籤：正負樣本比例約為 1:1\n",
    "        label = torch.FloatTensor([1.0 if np.random.random() > 0.5 else 0.0])\n",
    "        \n",
    "        return image_seq, label\n",
    "\n",
    "# 實例化模型\n",
    "model = EnhancedCNN_LSTM(num_classes=1, input_size=(128, 128)).to(device)\n",
    "\n",
    "# 檢查模型參數類型\n",
    "for param in model.parameters():\n",
    "    assert param.dtype == torch.float32, \"模型參數必須為 FP32\"\n",
    "\n",
    "# 聯合損失函數：結合 Focal Loss 和傳統 BCE Loss\n",
    "criterion = FocalLoss(alpha=0.25, gamma=2.0)\n",
    "\n",
    "# 優化器設定：使用 Lion 優化器，通常優於 AdamW\n",
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.parameters(),\n",
    "    lr=0.001,  # 稍高的初始學習率\n",
    "    weight_decay=2e-5,  # 輕微增加權重衰減以減少過擬合\n",
    "    betas=(0.9, 0.999)  # 預設動量值\n",
    ")\n",
    "\n",
    "# 進階學習率調度器：OneCycleLR 通常比 ReduceLROnPlateau 效果好\n",
    "scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=0.002,  # 最大學習率\n",
    "    total_steps=20 * (1152 // 4),  # epochs * batches_per_epoch\n",
    "    pct_start=0.3,  # 預熱階段佔總步數的比例\n",
    "    div_factor=10.0,  # 初始學習率 = max_lr / div_factor\n",
    "    final_div_factor=100.0  # 最終學習率 = max_lr / final_div_factor\n",
    ")\n",
    "\n",
    "# 混合精度訓練配置\n",
    "scaler = GradScaler(enabled=True)\n",
    "\n",
    "# 擴展資料增強管道\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.RandomResizedCrop(128, scale=(0.8, 1.0)),  # 隨機剪裁\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.1, hue=0.1),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 隨機平移\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet 標準化\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# 建立資料集和資料載入器\n",
    "train_dataset = SequenceDataset(num_samples=1152, timesteps=10, transform=train_transform, augment=True)\n",
    "val_dataset = SequenceDataset(num_samples=288, timesteps=10, transform=val_transform, augment=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "# 模型訓練設定\n",
    "num_epochs = 20\n",
    "accumulation_steps = 4  # 梯度累積以模擬更大批次\n",
    "\n",
    "# 輸入 F 模組\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 訓練記錄\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "patience = 5  # 提前停止的耐心值\n",
    "\n",
    "# 訓練迴圈\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    train_preds, train_labels = [], []\n",
    "    \n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 訓練階段\n",
    "    for i, (images, labels) in enumerate(tqdm(train_loader)):\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        # 前向傳播（使用混合精度）\n",
    "        with autocast(device_type='cuda'):\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss = loss / accumulation_steps  # 梯度累積\n",
    "        \n",
    "        # 反向傳播\n",
    "        scaler.scale(loss).backward()\n",
    "        \n",
    "        # 計算訓練指標\n",
    "        with torch.no_grad():\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            train_preds.extend(preds.flatten())\n",
    "            train_labels.extend(labels.cpu().numpy().flatten())\n",
    "        \n",
    "        # 批次累積更新\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
    "            # 梯度裁剪以防止梯度爆炸\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # 更新參數\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 更新學習率（每批次更新）\n",
    "            scheduler.step()\n",
    "        \n",
    "        running_loss += loss.item() * accumulation_steps\n",
    "    \n",
    "    # 計算平均訓練損失和準確率\n",
    "    avg_train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = accuracy_score(train_labels, train_preds) * 100\n",
    "    \n",
    "    # 驗證階段\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_preds, val_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32).view(-1, 1)\n",
    "            \n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            val_preds.extend(preds.flatten())\n",
    "            val_labels.extend(labels.cpu().numpy().flatten())\n",
    "    \n",
    "    # 計算驗證指標\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_accuracy = accuracy_score(val_labels, val_preds) * 100\n",
    "    \n",
    "    # 提前停止檢查\n",
    "    if val_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = val_accuracy\n",
    "        patience_counter = 0\n",
    "        # 儲存最佳模型\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_accuracy': val_accuracy,\n",
    "        }, 'best_enhanced_cnn_lstm.pth')\n",
    "        print(f'保存最佳模型，驗證準確率: {val_accuracy:.2f}%')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # 列印訓練資訊\n",
    "    print(f'第 {epoch+1}/{num_epochs} 個 Epoch')\n",
    "    print(f'訓練損失: {avg_train_loss:.4f}, 訓練準確率: {train_accuracy:.2f}%')\n",
    "    print(f'驗證損失: {avg_val_loss:.4f}, 驗證準確率: {val_accuracy:.2f}%')\n",
    "    print(f'學習率: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "    print(f'GPU記憶體使用量: {torch.cuda.memory_allocated() / 1024**3:.2f} GB')\n",
    "    \n",
    "    # # 提前停止\n",
    "    # if patience_counter >= patience:\n",
    "    #     print(f'驗證準確率已 {patience} 個 epoch 未改善，提前停止訓練')\n",
    "    #     break\n",
    "\n",
    "# 載入最佳模型進行最終評估\n",
    "# 使用 weights_only=True 以符合 PyTorch 未來的安全建議\n",
    "checkpoint = torch.load('best_enhanced_cnn_lstm.pth', weights_only=True, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# 最終驗證\n",
    "model.eval()\n",
    "final_preds, final_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in val_loader:\n",
    "        images = images.to(device, dtype=torch.float32)\n",
    "        labels = labels.to(device, dtype=torch.float32).view(-1, 1)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "        \n",
    "        final_preds.extend(preds.flatten())\n",
    "        final_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "final_accuracy = accuracy_score(final_labels, final_preds) * 100\n",
    "print(f'最終模型驗證準確率: {final_accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "創建資料集...\n",
      "訓練資料集大小: 1024\n",
      "驗證資料集大小: 256\n",
      "模型結構:\n",
      "DeepCNN_LSTM(\n",
      "  (conv_init): Sequential(\n",
      "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block1): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv_block3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace=True)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dropout1): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=8192, out_features=256, bias=True)\n",
      "  (bn_fc): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (lstm): LSTM(256, 128, batch_first=True, bidirectional=True)\n",
      "  (dropout2): Dropout(p=0.5, inplace=False)\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=64, bias=True)\n",
      "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): Dropout(p=0.3, inplace=False)\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "總參數數量: 2,801,489\n",
      "可訓練參數數量: 2,801,489\n",
      "開始訓練...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 128/128 [00:26<00:00,  4.83it/s]\n",
      "Epoch 1/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30:\n",
      "訓練損失: 0.4518, 訓練準確率: 96.48%\n",
      "驗證損失: 0.1885, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.000382\n",
      "保存最佳模型，驗證準確率: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.71it/s]\n",
      "Epoch 2/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30:\n",
      "訓練損失: 0.1667, 訓練準確率: 99.51%\n",
      "驗證損失: 0.0498, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.000616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 128/128 [00:26<00:00,  4.75it/s]\n",
      "Epoch 3/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30:\n",
      "訓練損失: 0.0983, 訓練準確率: 98.63%\n",
      "驗證損失: 0.0176, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.000976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.71it/s]\n",
      "Epoch 4/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30:\n",
      "訓練損失: 0.0703, 訓練準確率: 98.54%\n",
      "驗證損失: 0.0100, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.001417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.71it/s]\n",
      "Epoch 5/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30:\n",
      "訓練損失: 0.0236, 訓練準確率: 99.80%\n",
      "驗證損失: 0.0024, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.001886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.61it/s]\n",
      "Epoch 6/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30:\n",
      "訓練損失: 0.0271, 訓練準確率: 99.80%\n",
      "驗證損失: 0.0006, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.002327\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 128/128 [00:26<00:00,  4.84it/s]\n",
      "Epoch 7/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30:\n",
      "訓練損失: 0.0553, 訓練準確率: 98.93%\n",
      "驗證損失: 0.0003, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.002686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 128/128 [00:27<00:00,  4.63it/s]\n",
      "Epoch 8/30 [Val]: 100%|██████████| 32/32 [00:06<00:00,  4.94it/s]\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 33287 (\\N{CJK UNIFIED IDEOGRAPH-8207}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 35657 (\\N{CJK UNIFIED IDEOGRAPH-8B49}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:264: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 33287 (\\N{CJK UNIFIED IDEOGRAPH-8207}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 35657 (\\N{CJK UNIFIED IDEOGRAPH-8B49}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 28310 (\\N{CJK UNIFIED IDEOGRAPH-6E96}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 30906 (\\N{CJK UNIFIED IDEOGRAPH-78BA}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/484920737.py:265: UserWarning: Glyph 29575 (\\N{CJK UNIFIED IDEOGRAPH-7387}) missing from current font.\n",
      "  plt.savefig(save_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30:\n",
      "訓練損失: 0.0727, 訓練準確率: 99.22%\n",
      "驗證損失: 0.0001, 驗證準確率: 100.00%\n",
      "精確度: 1.0000, 召回率: 1.0000, F1: 1.0000\n",
      "學習率: 0.002920\n",
      "驗證準確率已 7 個 epoch 未改善，提前停止訓練\n",
      "訓練歷史已儲存至 training_history.png\n",
      "載入最佳模型進行最終評估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "最終評估: 100%|██████████| 32/32 [00:06<00:00,  4.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最終評估結果:\n",
      "準確率: 100.00%\n",
      "精確度: 1.0000\n",
      "召回率: 1.0000\n",
      "F1 分數: 1.0000\n",
      "訓練完成！最佳模型已保存。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 設置記憶體優化環境變數\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 深度優化版 CNN-LSTM 模型\n",
    "class DeepCNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=1, input_size=(128, 128)):\n",
    "        super(DeepCNN_LSTM, self).__init__()\n",
    "        \n",
    "        # 基礎配置\n",
    "        self.input_size = input_size\n",
    "        \n",
    "        # 初始卷積層 - 使用較小的卷積核和較少的通道數\n",
    "        self.conv_init = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 降到 64x64\n",
    "        )\n",
    "        \n",
    "        # 第一個雙重卷積區塊\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 降到 32x32\n",
    "        )\n",
    "        \n",
    "        # 第二個雙重卷積區塊\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 降到 16x16\n",
    "        )\n",
    "        \n",
    "        # 第三個雙重卷積區塊\n",
    "        self.conv_block3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 降到 8x8\n",
    "        )\n",
    "        \n",
    "        # 計算 CNN 輸出尺寸\n",
    "        self.cnn_output_size = 128 * 8 * 8  # 128 通道, 8x8 特徵圖\n",
    "        \n",
    "        # Dropout 和全連接層\n",
    "        self.dropout1 = nn.Dropout(0.5)  # 更高的 dropout 率\n",
    "        self.fc = nn.Linear(self.cnn_output_size, 256)\n",
    "        self.bn_fc = nn.BatchNorm1d(256)\n",
    "        \n",
    "        # LSTM 部分\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=256,\n",
    "            hidden_size=128,\n",
    "            num_layers=1,  # 減少層數\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 分類器\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(256, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 模型初始化\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"初始化模型權重\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d) or isinstance(m, nn.BatchNorm1d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 輸入形狀: [batch_size, timesteps, channels, height, width]\n",
    "        batch_size, timesteps, C, H, W = x.size()\n",
    "        \n",
    "        # 重塑並單獨處理每個時間步長\n",
    "        c_in = x.view(batch_size * timesteps, C, H, W)\n",
    "        \n",
    "        # 通過 CNN 特徵提取器\n",
    "        c_out = self.conv_init(c_in)\n",
    "        c_out = self.conv_block1(c_out)\n",
    "        c_out = self.conv_block2(c_out)\n",
    "        c_out = self.conv_block3(c_out)\n",
    "        \n",
    "        # 攤平\n",
    "        c_out = c_out.view(batch_size * timesteps, -1)\n",
    "        \n",
    "        # 應用 Dropout\n",
    "        c_out = self.dropout1(c_out)\n",
    "        \n",
    "        # 通過全連接層\n",
    "        c_out = self.fc(c_out)\n",
    "        c_out = self.bn_fc(c_out)\n",
    "        c_out = F.relu(c_out)\n",
    "        \n",
    "        # 重塑為序列\n",
    "        r_out = c_out.view(batch_size, timesteps, -1)\n",
    "        \n",
    "        # 通過 LSTM\n",
    "        r_out, _ = self.lstm(r_out)\n",
    "        \n",
    "        # 僅使用最後一個時間步的輸出\n",
    "        r_out = r_out[:, -1, :]\n",
    "        \n",
    "        # 應用 Dropout\n",
    "        r_out = self.dropout2(r_out)\n",
    "        \n",
    "        # 最終分類\n",
    "        output = self.classifier(r_out)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 設置自定義資料集類別\n",
    "class ImprovedSequenceDataset(Dataset):\n",
    "    def __init__(self, num_samples, timesteps=10, transform=None, seed=42):\n",
    "        \"\"\"\n",
    "        改進版資料集：產生更具鑑別度的資料\n",
    "        \n",
    "        Args:\n",
    "            num_samples: 樣本數量\n",
    "            timesteps: 每個樣本的時間步數\n",
    "            transform: 資料轉換\n",
    "            seed: 隨機種子，確保可重現性\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.timesteps = timesteps\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 設置隨機種子以確保可重現性\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "        # 預先產生標籤，確保正負樣本平衡\n",
    "        self.labels = np.array([0, 1] * (num_samples // 2 + 1))[:num_samples]\n",
    "        np.random.shuffle(self.labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # 獲取此索引的標籤\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # 建立時序圖像序列\n",
    "        images = []\n",
    "        \n",
    "        base_intensity = np.random.randint(100, 200)  # 基礎亮度\n",
    "        pattern_type = label  # 根據標籤決定模式類型\n",
    "        \n",
    "        for t in range(self.timesteps):\n",
    "            # 創建基礎圖像 - 灰度背景\n",
    "            img_array = np.ones((128, 128, 3), dtype=np.uint8) * base_intensity\n",
    "            \n",
    "            # 根據標籤添加不同的特徵模式\n",
    "            if pattern_type == 1:  # 正樣本\n",
    "                # 在圖像中心添加圓形\n",
    "                center_x, center_y = 64, 64\n",
    "                radius = 20 + int(10 * np.sin(t / 2))  # 隨時間變化的半徑\n",
    "                \n",
    "                # 繪製圓形\n",
    "                for i in range(128):\n",
    "                    for j in range(128):\n",
    "                        dist = np.sqrt((i - center_x)**2 + (j - center_y)**2)\n",
    "                        if dist < radius:\n",
    "                            # 紅色調圓形\n",
    "                            img_array[i, j, 0] = min(255, base_intensity + 80)  # R\n",
    "                            img_array[i, j, 1] = max(0, base_intensity - 50)    # G\n",
    "                            img_array[i, j, 2] = max(0, base_intensity - 50)    # B\n",
    "            else:  # 負樣本\n",
    "                # 添加矩形\n",
    "                start_x = 32 + int(10 * np.cos(t / 2))\n",
    "                start_y = 32 + int(10 * np.sin(t / 2))\n",
    "                width = height = 40\n",
    "                \n",
    "                # 繪製矩形\n",
    "                img_array[start_x:start_x+width, start_y:start_y+height, 0] = max(0, base_intensity - 50)\n",
    "                img_array[start_x:start_x+width, start_y:start_y+height, 1] = min(255, base_intensity + 70)\n",
    "                img_array[start_x:start_x+width, start_y:start_y+height, 2] = max(0, base_intensity - 30)\n",
    "            \n",
    "            # 添加一些噪聲\n",
    "            noise = np.random.randint(-20, 20, (128, 128, 3))\n",
    "            img_array = np.clip(img_array + noise, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # 轉換為 PIL 圖像\n",
    "            img = Image.fromarray(img_array)\n",
    "            \n",
    "            # 應用轉換\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            \n",
    "            images.append(img)\n",
    "        \n",
    "        # 堆疊為張量\n",
    "        image_seq = torch.stack(images)  # [timesteps, channels, height, width]\n",
    "        \n",
    "        return image_seq, torch.FloatTensor([float(label)])\n",
    "\n",
    "# 建立訓練圖表函數\n",
    "def plot_training_history(train_losses, val_losses, train_accs, val_accs, save_path='training_history.png'):\n",
    "    \"\"\"繪製訓練歷史圖表\"\"\"\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 損失曲線\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='訓練損失')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='驗證損失')\n",
    "    plt.title('訓練與驗證損失')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('損失')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 準確率曲線\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accs, 'b-', label='訓練準確率')\n",
    "    plt.plot(epochs, val_accs, 'r-', label='驗證準確率')\n",
    "    plt.title('訓練與驗證準確率')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('準確率 (%)')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"訓練歷史已儲存至 {save_path}\")\n",
    "\n",
    "# 訓練函數\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, num_epochs=25, patience=7):\n",
    "    \"\"\"完整的模型訓練函數\"\"\"\n",
    "    # 初始化\n",
    "    best_val_accuracy = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_accs, val_accs = [], []\n",
    "    scaler = GradScaler(enabled=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_preds, train_labels_list = [], []\n",
    "        \n",
    "        for images, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            labels = labels.to(device, dtype=torch.float32)\n",
    "            \n",
    "            # 清空梯度\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 前向傳播（使用混合精度）\n",
    "            with autocast(device_type='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            # 反向傳播\n",
    "            scaler.scale(loss).backward()\n",
    "            \n",
    "            # 梯度裁剪\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            # 更新權重\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            # 更新學習率\n",
    "            scheduler.step()\n",
    "            \n",
    "            # 統計\n",
    "            running_loss += loss.item()\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            train_preds.extend(preds)\n",
    "            train_labels_list.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 計算訓練指標\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = accuracy_score(train_labels_list, train_preds) * 100\n",
    "        train_losses.append(train_loss)\n",
    "        train_accs.append(train_accuracy)\n",
    "        \n",
    "        # 驗證階段\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_preds, val_labels_list = [], []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\"):\n",
    "                images = images.to(device, dtype=torch.float32)\n",
    "                labels = labels.to(device, dtype=torch.float32)\n",
    "                \n",
    "                # 前向傳播\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                # 統計\n",
    "                val_loss += loss.item()\n",
    "                preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "                val_preds.extend(preds)\n",
    "                val_labels_list.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 計算驗證指標\n",
    "        val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = accuracy_score(val_labels_list, val_preds) * 100\n",
    "        val_losses.append(val_loss)\n",
    "        val_accs.append(val_accuracy)\n",
    "        \n",
    "        # 計算精確度、召回率和 F1 分數\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            val_labels_list, val_preds, average='binary', zero_division=0\n",
    "        )\n",
    "        \n",
    "        # 輸出訓練資訊\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}:')\n",
    "        print(f'訓練損失: {train_loss:.4f}, 訓練準確率: {train_accuracy:.2f}%')\n",
    "        print(f'驗證損失: {val_loss:.4f}, 驗證準確率: {val_accuracy:.2f}%')\n",
    "        print(f'精確度: {precision:.4f}, 召回率: {recall:.4f}, F1: {f1:.4f}')\n",
    "        print(f'學習率: {optimizer.param_groups[0][\"lr\"]:.6f}')\n",
    "        \n",
    "        # 檢查是否是最佳模型\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # 儲存最佳模型\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'val_precision': precision,\n",
    "                'val_recall': recall,\n",
    "                'val_f1': f1\n",
    "            }, 'best_deep_cnn_lstm.pth')\n",
    "            \n",
    "            print(f'保存最佳模型，驗證準確率: {val_accuracy:.2f}%')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # 提前停止\n",
    "        if patience_counter >= patience:\n",
    "            print(f'驗證準確率已 {patience} 個 epoch 未改善，提前停止訓練')\n",
    "            break\n",
    "    \n",
    "    # 繪製訓練歷史\n",
    "    plot_training_history(train_losses, val_losses, train_accs, val_accs)\n",
    "    \n",
    "    return train_losses, val_losses, train_accs, val_accs\n",
    "\n",
    "# 主程式：設置訓練\n",
    "def main():\n",
    "    # 模型配置\n",
    "    input_size = (128, 128)\n",
    "    num_classes = 1\n",
    "    batch_size = 8\n",
    "    num_epochs = 30\n",
    "    \n",
    "    # 設置資料轉換\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 創建改進的資料集\n",
    "    print(\"創建資料集...\")\n",
    "    train_dataset = ImprovedSequenceDataset(num_samples=1024, timesteps=10, transform=train_transform, seed=42)\n",
    "    val_dataset = ImprovedSequenceDataset(num_samples=256, timesteps=10, transform=val_transform, seed=43)\n",
    "    \n",
    "    # 資料載入器\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    print(f\"訓練資料集大小: {len(train_dataset)}\")\n",
    "    print(f\"驗證資料集大小: {len(val_dataset)}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = DeepCNN_LSTM(num_classes=num_classes, input_size=input_size).to(device)\n",
    "    print(\"模型結構:\")\n",
    "    print(model)\n",
    "    \n",
    "    # 訓練前打印模型參數數量\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"總參數數量: {total_params:,}\")\n",
    "    print(f\"可訓練參數數量: {trainable_params:,}\")\n",
    "    \n",
    "    # 損失函數、優化器和學習率調度器\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "    \n",
    "    # 使用 OneCycleLR 調度器\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=0.003,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=10.0,\n",
    "        final_div_factor=100.0\n",
    "    )\n",
    "    \n",
    "    # 開始訓練\n",
    "    print(\"開始訓練...\")\n",
    "    train_losses, val_losses, train_accs, val_accs = train_model(\n",
    "        model, train_loader, val_loader, optimizer, criterion, scheduler, \n",
    "        num_epochs=num_epochs, patience=7\n",
    "    )\n",
    "    \n",
    "    # 載入最佳模型進行最終評估\n",
    "    print(\"載入最佳模型進行最終評估...\")\n",
    "    best_model = DeepCNN_LSTM(num_classes=num_classes, input_size=input_size).to(device)\n",
    "    checkpoint = torch.load('best_deep_cnn_lstm.pth', weights_only=False, map_location=device)\n",
    "    best_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 最終評估\n",
    "    best_model.eval()\n",
    "    final_preds, final_labels = [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader, desc=\"最終評估\"):\n",
    "            images = images.to(device, dtype=torch.float32)\n",
    "            outputs = best_model(images)\n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            final_preds.extend(preds)\n",
    "            final_labels.extend(labels.numpy())\n",
    "    \n",
    "    # 計算最終指標\n",
    "    final_accuracy = accuracy_score(final_labels, final_preds) * 100\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        final_labels, final_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # 輸出最終結果\n",
    "    print(\"\\n最終評估結果:\")\n",
    "    print(f\"準確率: {final_accuracy:.2f}%\")\n",
    "    print(f\"精確度: {precision:.4f}\")\n",
    "    print(f\"召回率: {recall:.4f}\")\n",
    "    print(f\"F1 分數: {f1:.4f}\")\n",
    "    \n",
    "    # 保存完整模型\n",
    "    torch.save(best_model, 'complete_deep_cnn_lstm.pth')\n",
    "    print(\"訓練完成！最佳模型已保存。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用設備: cuda\n",
      "載入資料集...\n",
      "找到 180 個序列資料夾\n",
      "創建了 2388 個樣本 (正樣本: 1194, 負樣本: 1194)\n",
      "訓練數據集大小: 1910\n",
      "驗證數據集大小: 478\n",
      "總參數數量: 14,470,594\n",
      "可訓練參數數量: 13,787,522\n",
      "開始訓練...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.03it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 | LR: 0.000636\n",
      "Train - Loss: 0.6639, Acc: 61.43%, F1: 0.5978\n",
      "Valid - Loss: 0.5115, Acc: 78.24%, F1: 0.7668\n",
      "Valid - Precision: 0.8261, Recall: 0.7155\n",
      "Confusion Matrix:\n",
      "[[203  36]\n",
      " [ 68 171]]\n",
      "儲存最佳模型 (F1: 0.7668)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.47it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30 | LR: 0.001027\n",
      "Train - Loss: 0.6508, Acc: 67.82%, F1: 0.6984\n",
      "Valid - Loss: 0.5685, Acc: 75.73%, F1: 0.7836\n",
      "Valid - Precision: 0.7071, Recall: 0.8787\n",
      "Confusion Matrix:\n",
      "[[152  87]\n",
      " [ 29 210]]\n",
      "儲存最佳模型 (F1: 0.7836)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.63it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 25.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30 | LR: 0.001625\n",
      "Train - Loss: 0.6024, Acc: 73.27%, F1: 0.7363\n",
      "Valid - Loss: 0.8429, Acc: 66.95%, F1: 0.7331\n",
      "Valid - Precision: 0.6147, Recall: 0.9079\n",
      "Confusion Matrix:\n",
      "[[103 136]\n",
      " [ 22 217]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.13it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30 | LR: 0.002360\n",
      "Train - Loss: 0.6475, Acc: 67.03%, F1: 0.6538\n",
      "Valid - Loss: 0.5994, Acc: 70.92%, F1: 0.6775\n",
      "Valid - Precision: 0.7604, Recall: 0.6109\n",
      "Confusion Matrix:\n",
      "[[193  46]\n",
      " [ 93 146]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.33it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30 | LR: 0.003142\n",
      "Train - Loss: 0.6745, Acc: 62.63%, F1: 0.6241\n",
      "Valid - Loss: 0.6393, Acc: 69.04%, F1: 0.7087\n",
      "Valid - Precision: 0.6691, Recall: 0.7531\n",
      "Confusion Matrix:\n",
      "[[150  89]\n",
      " [ 59 180]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.23it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 25.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30 | LR: 0.003876\n",
      "Train - Loss: 0.6580, Acc: 64.99%, F1: 0.6735\n",
      "Valid - Loss: 0.6047, Acc: 69.25%, F1: 0.6621\n",
      "Valid - Precision: 0.7347, Recall: 0.6025\n",
      "Confusion Matrix:\n",
      "[[187  52]\n",
      " [ 95 144]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.00it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30 | LR: 0.004474\n",
      "Train - Loss: 0.6385, Acc: 69.03%, F1: 0.7064\n",
      "Valid - Loss: 0.5330, Acc: 76.57%, F1: 0.7812\n",
      "Valid - Precision: 0.7326, Recall: 0.8368\n",
      "Confusion Matrix:\n",
      "[[166  73]\n",
      " [ 39 200]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.14it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30 | LR: 0.004865\n",
      "Train - Loss: 0.6234, Acc: 69.39%, F1: 0.7179\n",
      "Valid - Loss: 0.5860, Acc: 70.92%, F1: 0.7301\n",
      "Valid - Precision: 0.6812, Recall: 0.7866\n",
      "Confusion Matrix:\n",
      "[[151  88]\n",
      " [ 51 188]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.19it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30 | LR: 0.005000\n",
      "Train - Loss: 0.5735, Acc: 76.05%, F1: 0.7674\n",
      "Valid - Loss: 0.5349, Acc: 76.78%, F1: 0.8021\n",
      "Valid - Precision: 0.6988, Recall: 0.9414\n",
      "Confusion Matrix:\n",
      "[[142  97]\n",
      " [ 14 225]]\n",
      "儲存最佳模型 (F1: 0.8021)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.16it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30 | LR: 0.004972\n",
      "Train - Loss: 0.5053, Acc: 80.14%, F1: 0.8096\n",
      "Valid - Loss: 0.3971, Acc: 85.98%, F1: 0.8613\n",
      "Valid - Precision: 0.8525, Recall: 0.8703\n",
      "Confusion Matrix:\n",
      "[[203  36]\n",
      " [ 31 208]]\n",
      "儲存最佳模型 (F1: 0.8613)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.42it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30 | LR: 0.004889\n",
      "Train - Loss: 0.4387, Acc: 84.59%, F1: 0.8521\n",
      "Valid - Loss: 0.3484, Acc: 88.91%, F1: 0.8898\n",
      "Valid - Precision: 0.8843, Recall: 0.8954\n",
      "Confusion Matrix:\n",
      "[[211  28]\n",
      " [ 25 214]]\n",
      "儲存最佳模型 (F1: 0.8898)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.07it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30 | LR: 0.004752\n",
      "Train - Loss: 0.3732, Acc: 87.26%, F1: 0.8752\n",
      "Valid - Loss: 0.2143, Acc: 94.56%, F1: 0.9444\n",
      "Valid - Precision: 0.9651, Recall: 0.9247\n",
      "Confusion Matrix:\n",
      "[[231   8]\n",
      " [ 18 221]]\n",
      "儲存最佳模型 (F1: 0.9444)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.46it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30 | LR: 0.004566\n",
      "Train - Loss: 0.3354, Acc: 89.26%, F1: 0.8957\n",
      "Valid - Loss: 0.1775, Acc: 95.19%, F1: 0.9530\n",
      "Valid - Precision: 0.9320, Recall: 0.9749\n",
      "Confusion Matrix:\n",
      "[[222  17]\n",
      " [  6 233]]\n",
      "儲存最佳模型 (F1: 0.9530)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.05it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30 | LR: 0.004333\n",
      "Train - Loss: 0.2662, Acc: 91.46%, F1: 0.9170\n",
      "Valid - Loss: 0.2195, Acc: 94.56%, F1: 0.9469\n",
      "Valid - Precision: 0.9243, Recall: 0.9707\n",
      "Confusion Matrix:\n",
      "[[220  19]\n",
      " [  7 232]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.07it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30 | LR: 0.004059\n",
      "Train - Loss: 0.2698, Acc: 92.30%, F1: 0.9256\n",
      "Valid - Loss: 0.1109, Acc: 97.07%, F1: 0.9706\n",
      "Valid - Precision: 0.9747, Recall: 0.9665\n",
      "Confusion Matrix:\n",
      "[[233   6]\n",
      " [  8 231]]\n",
      "儲存最佳模型 (F1: 0.9706)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.28it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30 | LR: 0.003751\n",
      "Train - Loss: 0.1984, Acc: 94.55%, F1: 0.9468\n",
      "Valid - Loss: 0.1082, Acc: 97.07%, F1: 0.9713\n",
      "Valid - Precision: 0.9518, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[227  12]\n",
      " [  2 237]]\n",
      "儲存最佳模型 (F1: 0.9713)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.18it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30 | LR: 0.003414\n",
      "Train - Loss: 0.1747, Acc: 95.07%, F1: 0.9519\n",
      "Valid - Loss: 0.0823, Acc: 97.91%, F1: 0.9791\n",
      "Valid - Precision: 0.9791, Recall: 0.9791\n",
      "Confusion Matrix:\n",
      "[[234   5]\n",
      " [  5 234]]\n",
      "儲存最佳模型 (F1: 0.9791)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.18it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30 | LR: 0.003057\n",
      "Train - Loss: 0.1802, Acc: 94.97%, F1: 0.9509\n",
      "Valid - Loss: 0.0761, Acc: 98.54%, F1: 0.9853\n",
      "Valid - Precision: 0.9874, Recall: 0.9833\n",
      "Confusion Matrix:\n",
      "[[236   3]\n",
      " [  4 235]]\n",
      "儲存最佳模型 (F1: 0.9853)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.13it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30 | LR: 0.002688\n",
      "Train - Loss: 0.1552, Acc: 96.28%, F1: 0.9633\n",
      "Valid - Loss: 0.1437, Acc: 96.65%, F1: 0.9654\n",
      "Valid - Precision: 1.0000, Recall: 0.9331\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [ 16 223]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.15it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30 | LR: 0.002315\n",
      "Train - Loss: 0.1456, Acc: 96.12%, F1: 0.9614\n",
      "Valid - Loss: 0.0505, Acc: 98.12%, F1: 0.9810\n",
      "Valid - Precision: 0.9915, Recall: 0.9707\n",
      "Confusion Matrix:\n",
      "[[237   2]\n",
      " [  7 232]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.11it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30 | LR: 0.001946\n",
      "Train - Loss: 0.1257, Acc: 96.70%, F1: 0.9675\n",
      "Valid - Loss: 0.1019, Acc: 97.70%, F1: 0.9772\n",
      "Valid - Precision: 0.9672, Recall: 0.9874\n",
      "Confusion Matrix:\n",
      "[[231   8]\n",
      " [  3 236]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.28it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30 | LR: 0.001589\n",
      "Train - Loss: 0.1061, Acc: 97.33%, F1: 0.9736\n",
      "Valid - Loss: 0.0670, Acc: 98.33%, F1: 0.9834\n",
      "Valid - Precision: 0.9753, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[233   6]\n",
      " [  2 237]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.06it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30 | LR: 0.001253\n",
      "Train - Loss: 0.0652, Acc: 98.53%, F1: 0.9854\n",
      "Valid - Loss: 0.0526, Acc: 98.54%, F1: 0.9854\n",
      "Valid - Precision: 0.9793, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[234   5]\n",
      " [  2 237]]\n",
      "儲存最佳模型 (F1: 0.9854)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.06it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30 | LR: 0.000945\n",
      "Train - Loss: 0.0669, Acc: 98.58%, F1: 0.9859\n",
      "Valid - Loss: 0.0300, Acc: 99.37%, F1: 0.9937\n",
      "Valid - Precision: 1.0000, Recall: 0.9874\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  3 236]]\n",
      "儲存最佳模型 (F1: 0.9937)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.12it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30 | LR: 0.000671\n",
      "Train - Loss: 0.0443, Acc: 99.16%, F1: 0.9916\n",
      "Valid - Loss: 0.0240, Acc: 99.58%, F1: 0.9958\n",
      "Valid - Precision: 1.0000, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "儲存最佳模型 (F1: 0.9958)\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.22it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30 | LR: 0.000439\n",
      "Train - Loss: 0.0458, Acc: 98.95%, F1: 0.9896\n",
      "Valid - Loss: 0.0250, Acc: 99.58%, F1: 0.9958\n",
      "Valid - Precision: 1.0000, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.28it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 28.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30 | LR: 0.000252\n",
      "Train - Loss: 0.0290, Acc: 99.21%, F1: 0.9922\n",
      "Valid - Loss: 0.0312, Acc: 99.58%, F1: 0.9958\n",
      "Valid - Precision: 1.0000, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 29.21it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 27.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30 | LR: 0.000116\n",
      "Train - Loss: 0.0307, Acc: 99.42%, F1: 0.9942\n",
      "Valid - Loss: 0.0280, Acc: 99.58%, F1: 0.9958\n",
      "Valid - Precision: 1.0000, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 [Train]: 100%|██████████| 477/477 [00:17<00:00, 27.05it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 26.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30 | LR: 0.000033\n",
      "Train - Loss: 0.0223, Acc: 99.53%, F1: 0.9953\n",
      "Valid - Loss: 0.0385, Acc: 99.37%, F1: 0.9937\n",
      "Valid - Precision: 1.0000, Recall: 0.9874\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  3 236]]\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 [Train]: 100%|██████████| 477/477 [00:16<00:00, 28.88it/s]\n",
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 25.92it/s]\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 33287 (\\N{CJK UNIFIED IDEOGRAPH-8207}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:387: UserWarning: Glyph 35657 (\\N{CJK UNIFIED IDEOGRAPH-8B49}) missing from current font.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 25613 (\\N{CJK UNIFIED IDEOGRAPH-640D}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 22833 (\\N{CJK UNIFIED IDEOGRAPH-5931}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 35347 (\\N{CJK UNIFIED IDEOGRAPH-8A13}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 32244 (\\N{CJK UNIFIED IDEOGRAPH-7DF4}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 33287 (\\N{CJK UNIFIED IDEOGRAPH-8207}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 39511 (\\N{CJK UNIFIED IDEOGRAPH-9A57}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:388: UserWarning: Glyph 35657 (\\N{CJK UNIFIED IDEOGRAPH-8B49}) missing from current font.\n",
      "  plt.savefig(save_path)\n",
      "/tmp/ipykernel_3250/1904468619.py:530: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('best_traffic_model.pth', map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30 | LR: 0.000005\n",
      "Train - Loss: 0.0236, Acc: 99.48%, F1: 0.9948\n",
      "Valid - Loss: 0.0298, Acc: 99.58%, F1: 0.9958\n",
      "Valid - Precision: 1.0000, Recall: 0.9916\n",
      "Confusion Matrix:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "------------------------------------------------------------\n",
      "訓練歷史已儲存至 training_history.png\n",
      "載入最佳模型進行最終評估...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "評估中: 100%|██████████| 120/120 [00:04<00:00, 25.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "最終評估結果:\n",
      "準確率: 99.58%\n",
      "精確度: 1.0000\n",
      "召回率: 0.9916\n",
      "F1 分數: 0.9958\n",
      "混淆矩陣:\n",
      "[[239   0]\n",
      " [  2 237]]\n",
      "訓練完成！所有模型檔案已保存。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from torch.amp import GradScaler, autocast\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 設置隨機種子，確保可重現性\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# 設置記憶體優化環境變數\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "# 設置設備\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"使用設備: {device}\")\n",
    "\n",
    "# 改進版 CNN-LSTM 模型\n",
    "class ImprovedCNN_LSTM(nn.Module):\n",
    "    def __init__(self, num_classes=1, seq_length=10):\n",
    "        super(ImprovedCNN_LSTM, self).__init__()\n",
    "        \n",
    "        # 使用預訓練的 ResNet-18 作為特徵提取器\n",
    "        # 移除最後的全連接層\n",
    "        from torchvision.models import resnet18, ResNet18_Weights\n",
    "        self.feature_extractor = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "        self.feature_size = self.feature_extractor.fc.in_features  # 通常是 512\n",
    "        self.feature_extractor.fc = nn.Identity()  # 移除最後的全連接層\n",
    "        \n",
    "        # 凍結前幾層以避免過擬合\n",
    "        layers_to_freeze = list(self.feature_extractor.children())[:6]  # 凍結前6層\n",
    "        for layer in layers_to_freeze:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # LSTM 部分\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.feature_size,\n",
    "            hidden_size=256,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "            dropout=0.5,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        \n",
    "        # 注意力機制\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(512, 128),  # 512 = 256*2 (雙向LSTM)\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # 分類器\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.LayerNorm(128),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.LayerNorm(64),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, num_classes)\n",
    "        )\n",
    "        \n",
    "        # 時間序列長度\n",
    "        self.seq_length = seq_length\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, C, H, W = x.size()\n",
    "        \n",
    "        # 展平時間維度，作為批次處理\n",
    "        x = x.view(batch_size * seq_len, C, H, W)\n",
    "        \n",
    "        # 提取特徵\n",
    "        x = self.feature_extractor(x)  # 輸出: [batch_size*seq_len, feature_size]\n",
    "        \n",
    "        # 重塑為序列格式\n",
    "        x = x.view(batch_size, seq_len, -1)  # [batch_size, seq_len, feature_size]\n",
    "        \n",
    "        # 通過 LSTM 處理序列\n",
    "        lstm_out, _ = self.lstm(x)  # [batch_size, seq_len, hidden_size*2]\n",
    "        \n",
    "        # 應用注意力機制\n",
    "        attention_weights = self.attention(lstm_out)  # [batch_size, seq_len, 1]\n",
    "        context_vector = torch.sum(attention_weights * lstm_out, dim=1)  # [batch_size, hidden_size*2]\n",
    "        \n",
    "        # 最終分類\n",
    "        output = self.classifier(context_vector)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# 真實交通數據集\n",
    "class TrafficDataset(Dataset):\n",
    "    def __init__(self, root_dir, sequence_length=10, transform=None, max_sequences=None, mode='train'):\n",
    "        \"\"\"\n",
    "        真實交通數據集\n",
    "        \n",
    "        Args:\n",
    "            root_dir (str): 資料夾路徑，包含多個子資料夾，每個子資料夾是一個序列\n",
    "            sequence_length (int): 每個樣本的時間步數\n",
    "            transform: 圖像轉換\n",
    "            max_sequences (int, optional): 限制使用的序列數量\n",
    "            mode (str): 'train' 或 'test'，影響資料增強和處理\n",
    "        \"\"\"\n",
    "        self.root_dir = root_dir\n",
    "        self.sequence_length = sequence_length\n",
    "        self.transform = transform\n",
    "        self.mode = mode\n",
    "        \n",
    "        # 獲取所有序列資料夾\n",
    "        self.sequence_folders = sorted([\n",
    "            f for f in os.listdir(root_dir) \n",
    "            if os.path.isdir(os.path.join(root_dir, f)) and f.startswith('freeway_')\n",
    "        ])\n",
    "        \n",
    "        if max_sequences and max_sequences < len(self.sequence_folders):\n",
    "            # 隨機選擇子集\n",
    "            random.shuffle(self.sequence_folders)\n",
    "            self.sequence_folders = self.sequence_folders[:max_sequences]\n",
    "            \n",
    "        print(f\"找到 {len(self.sequence_folders)} 個序列資料夾\")\n",
    "        \n",
    "        # 為每個序列加載標籤（這裡需要根據實際情況調整）\n",
    "        self.sequence_labels = {}\n",
    "        self.sequences = []\n",
    "        \n",
    "        for folder in self.sequence_folders:\n",
    "            folder_path = os.path.join(root_dir, folder)\n",
    "            \n",
    "            # 獲取所有圖像並按數字排序\n",
    "            images = sorted([\n",
    "                f for f in os.listdir(folder_path) \n",
    "                if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "            ], key=lambda x: int(x.split('.')[0]) if x.split('.')[0].isdigit() else 0)\n",
    "            \n",
    "            # 如果圖像太少，跳過\n",
    "            if len(images) < sequence_length:\n",
    "                continue\n",
    "                \n",
    "            # 為每個可能的子序列創建一個樣本\n",
    "            for i in range(0, len(images) - sequence_length + 1, sequence_length // 2):  # 使用重疊窗口\n",
    "                sub_sequence = images[i:i+sequence_length]\n",
    "                if len(sub_sequence) == sequence_length:\n",
    "                    # 這裡需要為每個子序列分配標籤\n",
    "                    # 例如，可以基於圖像特徵或文件名來確定標籤\n",
    "                    # 對於演示，我們使用簡單的啟發式方法：\n",
    "                    # 如果文件夾名稱的數字是奇數，標記為正樣本，否則為負樣本\n",
    "                    folder_id = int(folder.split('_')[1])\n",
    "                    label = 1 if folder_id % 2 == 1 else 0  # 奇數為正樣本\n",
    "                    \n",
    "                    self.sequences.append({\n",
    "                        'folder': folder_path,\n",
    "                        'images': sub_sequence,\n",
    "                        'label': label\n",
    "                    })\n",
    "        \n",
    "        # 確保類別平衡\n",
    "        positive_samples = [s for s in self.sequences if s['label'] == 1]\n",
    "        negative_samples = [s for s in self.sequences if s['label'] == 0]\n",
    "        \n",
    "        # 通過下采樣較多的類別來平衡數據集\n",
    "        min_samples = min(len(positive_samples), len(negative_samples))\n",
    "        \n",
    "        if len(positive_samples) > min_samples:\n",
    "            positive_samples = random.sample(positive_samples, min_samples)\n",
    "        if len(negative_samples) > min_samples:\n",
    "            negative_samples = random.sample(negative_samples, min_samples)\n",
    "            \n",
    "        self.sequences = positive_samples + negative_samples\n",
    "        random.shuffle(self.sequences)\n",
    "        \n",
    "        print(f\"創建了 {len(self.sequences)} 個樣本 (正樣本: {len(positive_samples)}, 負樣本: {len(negative_samples)})\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sequence_info = self.sequences[idx]\n",
    "        folder_path = sequence_info['folder']\n",
    "        image_names = sequence_info['images']\n",
    "        label = sequence_info['label']\n",
    "        \n",
    "        # 加載並轉換圖像序列\n",
    "        images = []\n",
    "        for img_name in image_names:\n",
    "            img_path = os.path.join(folder_path, img_name)\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            \n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "                \n",
    "            images.append(image)\n",
    "        \n",
    "        # 堆疊為單一張量\n",
    "        image_sequence = torch.stack(images)  # [sequence_length, C, H, W]\n",
    "        \n",
    "        return image_sequence, torch.FloatTensor([float(label)])\n",
    "\n",
    "# 評估函數\n",
    "def evaluate_model(model, data_loader, criterion):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(data_loader, desc=\"評估中\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # 計算評估指標\n",
    "    accuracy = accuracy_score(all_labels, all_preds) * 100\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_labels, all_preds, average='binary', zero_division=0\n",
    "    )\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    metrics = {\n",
    "        'loss': total_loss / len(data_loader),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# 訓練函數\n",
    "def train_model(model, train_loader, val_loader, optimizer, criterion, scheduler, \n",
    "               num_epochs=30, patience=7, checkpoint_path='best_traffic_model.pth'):\n",
    "    # 初始化變數\n",
    "    best_val_f1 = 0.0\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "    train_f1s, val_f1s = [], []\n",
    "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # 訓練階段\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        all_train_preds = []\n",
    "        all_train_labels = []\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 修正 autocast 用法，加入必要的 device_type 參數\n",
    "            device_type = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "            with autocast(device_type=device_type, enabled=torch.cuda.is_available()):\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "            \n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            \n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            preds = (torch.sigmoid(outputs) > 0.5).float().cpu().numpy()\n",
    "            all_train_preds.extend(preds)\n",
    "            all_train_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # 計算訓練指標\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(\n",
    "            all_train_labels, all_train_preds, average='binary', zero_division=0\n",
    "        )\n",
    "        train_accuracy = accuracy_score(all_train_labels, all_train_preds) * 100\n",
    "        \n",
    "        # 驗證階段\n",
    "        val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "        \n",
    "        # 記錄指標\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_metrics['loss'])\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_metrics['f1'])\n",
    "        \n",
    "        # 打印訓練資訊\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_accuracy:.2f}%, F1: {train_f1:.4f}\")\n",
    "        print(f\"Valid - Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.2f}%, F1: {val_metrics['f1']:.4f}\")\n",
    "        print(f\"Valid - Precision: {val_metrics['precision']:.4f}, Recall: {val_metrics['recall']:.4f}\")\n",
    "        print(f\"Confusion Matrix:\\n{val_metrics['confusion_matrix']}\")\n",
    "        \n",
    "        # 檢查是否為最佳模型\n",
    "        if val_metrics['f1'] > best_val_f1:\n",
    "            best_val_f1 = val_metrics['f1']\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # 保存最佳模型\n",
    "            torch.save({\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_f1': best_val_f1,\n",
    "                'accuracy': val_metrics['accuracy'],\n",
    "                'precision': val_metrics['precision'],\n",
    "                'recall': val_metrics['recall']\n",
    "            }, checkpoint_path)\n",
    "            \n",
    "            print(f\"儲存最佳模型 (F1: {best_val_f1:.4f})\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # 提前停止\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"驗證 F1 已 {patience} 個 epoch 未改善，提前停止訓練\")\n",
    "            break\n",
    "            \n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    # 繪製訓練歷史\n",
    "    plot_training_history(train_losses, val_losses, train_f1s, val_f1s, \n",
    "                        save_path='training_history.png', metric_name='F1 Score')\n",
    "    \n",
    "    return train_losses, val_losses, train_f1s, val_f1s\n",
    "\n",
    "# 繪製訓練歷史\n",
    "def plot_training_history(train_losses, val_losses, train_metrics, val_metrics, \n",
    "                         save_path='training_history.png', metric_name='Accuracy'):\n",
    "    epochs = range(1, len(train_losses) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # 損失曲線\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, 'b-', label='訓練損失')\n",
    "    plt.plot(epochs, val_losses, 'r-', label='驗證損失')\n",
    "    plt.title('訓練與驗證損失')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('損失')\n",
    "    plt.legend()\n",
    "    \n",
    "    # 指標曲線\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_metrics, 'b-', label=f'訓練 {metric_name}')\n",
    "    plt.plot(epochs, val_metrics, 'r-', label=f'驗證 {metric_name}')\n",
    "    plt.title(f'訓練與驗證 {metric_name}')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    \n",
    "    print(f\"訓練歷史已儲存至 {save_path}\")\n",
    "\n",
    "# 主程式\n",
    "def main():\n",
    "    # 配置參數\n",
    "    data_root = \"train/train\"  # 根據實際路徑調整\n",
    "    sequence_length = 10\n",
    "    batch_size = 4\n",
    "    num_epochs = 30\n",
    "    learning_rate = 0.0005\n",
    "    num_classes = 1\n",
    "    \n",
    "    # 圖像轉換\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # 載入資料集\n",
    "    print(\"載入資料集...\")\n",
    "    \n",
    "    # 檢查數據集路徑是否存在\n",
    "    if not os.path.exists(data_root):\n",
    "        potential_paths = [\n",
    "            \"train/train\",\n",
    "            \"train\",\n",
    "            \"data/train\",\n",
    "            \"dataset/train\",\n",
    "            \"../train/train\",\n",
    "            \"../train\"\n",
    "        ]\n",
    "        \n",
    "        for path in potential_paths:\n",
    "            if os.path.exists(path):\n",
    "                data_root = path\n",
    "                print(f\"找到數據集路徑: {data_root}\")\n",
    "                break\n",
    "        else:\n",
    "            print(\"無法找到數據集路徑，請手動指定\")\n",
    "            return\n",
    "    \n",
    "    # 創建完整數據集\n",
    "    full_dataset = TrafficDataset(root_dir=data_root, \n",
    "                                sequence_length=sequence_length, \n",
    "                                transform=None,  # 先不應用轉換\n",
    "                                max_sequences=None)  # 使用所有數據\n",
    "    \n",
    "    # 分割數據集\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    \n",
    "    train_indices, val_indices = train_test_split(\n",
    "        range(len(full_dataset)), \n",
    "        test_size=val_size/len(full_dataset),\n",
    "        random_state=42,\n",
    "        stratify=[s['label'] for s in full_dataset.sequences]  # 確保分層抽樣\n",
    "    )\n",
    "    \n",
    "    # 創建訓練和驗證數據集\n",
    "    train_dataset = torch.utils.data.Subset(full_dataset, train_indices)\n",
    "    val_dataset = torch.utils.data.Subset(full_dataset, val_indices)\n",
    "    \n",
    "    # 添加數據轉換\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = val_transform\n",
    "    \n",
    "    # 數據加載器\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True, \n",
    "        num_workers=4, \n",
    "        pin_memory=True,\n",
    "        drop_last=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=False, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"訓練數據集大小: {len(train_dataset)}\")\n",
    "    print(f\"驗證數據集大小: {len(val_dataset)}\")\n",
    "    \n",
    "    # 初始化模型\n",
    "    model = ImprovedCNN_LSTM(num_classes=num_classes, seq_length=sequence_length).to(device)\n",
    "    \n",
    "    # 統計模型參數\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"總參數數量: {total_params:,}\")\n",
    "    print(f\"可訓練參數數量: {trainable_params:,}\")\n",
    "    \n",
    "    # 初始化損失函數和優化器\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "    \n",
    "    # 學習率調度器\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=learning_rate * 10,\n",
    "        epochs=num_epochs,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        pct_start=0.3,\n",
    "        anneal_strategy='cos',\n",
    "        div_factor=10.0,\n",
    "        final_div_factor=100.0\n",
    "    )\n",
    "    \n",
    "    # 開始訓練\n",
    "    print(\"開始訓練...\")\n",
    "    train_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        optimizer=optimizer,\n",
    "        criterion=criterion,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=num_epochs,\n",
    "        patience=7,\n",
    "        checkpoint_path='best_traffic_model.pth'\n",
    "    )\n",
    "    \n",
    "    # 載入最佳模型進行最終評估\n",
    "    print(\"載入最佳模型進行最終評估...\")\n",
    "    checkpoint = torch.load('best_traffic_model.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # 最終評估\n",
    "    val_metrics = evaluate_model(model, val_loader, criterion)\n",
    "    \n",
    "    # 輸出最終結果\n",
    "    print(\"\\n最終評估結果:\")\n",
    "    print(f\"準確率: {val_metrics['accuracy']:.2f}%\")\n",
    "    print(f\"精確度: {val_metrics['precision']:.4f}\")\n",
    "    print(f\"召回率: {val_metrics['recall']:.4f}\")\n",
    "    print(f\"F1 分數: {val_metrics['f1']:.4f}\")\n",
    "    print(f\"混淆矩陣:\\n{val_metrics['confusion_matrix']}\")\n",
    "    \n",
    "    # 保存完整模型，使用 state_dict 保存以便於移植性\n",
    "    torch.save(model.state_dict(), 'traffic_model_state_dict.pth')\n",
    "    \n",
    "    # 保存 scriptable 模型版本 (TorchScript)，便於部署\n",
    "    scripted_model = torch.jit.script(model.cpu())\n",
    "    scripted_model.save('traffic_model_scripted.pt')\n",
    "    \n",
    "    print(\"訓練完成！所有模型檔案已保存。\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K-YKJ0FsVt95"
   },
   "source": [
    " 評估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Ec89bL2LVs5R",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 63.888888888888886%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to('cuda'), labels.to('cuda').float().view(-1, 1)\n",
    "        outputs = model(images)\n",
    "        predicted = (outputs > 0.5).float()\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNwqSFzx3n/xj3LtAy6XmKw",
   "gpuType": "T4",
   "mount_file_id": "1nfo3NhRZgb_QE0A4BoNU2I299qMAMlKF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
